{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate user requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data_generation\n",
    "from langchain_community.chat_models import ChatFireworks\n",
    "from langchain_core.output_parsers import NumberedListOutputParser\n",
    "\n",
    "\n",
    "system_generate_prompt = \"\"\"\n",
    "You create a dataset of calls to a chatbot that can book hotels\n",
    "\n",
    "Generate 20 different requests. \n",
    "Each request may or may not include different content: destination, check-in and check-out time, number of adults and children or just guests, various additional information, wishes about the rating of the hotel (according to a 10-point system) and about the stars of the hotel, about the price, etc.\n",
    "\n",
    "a few examples:\n",
    "1. Find me cheap hotels in Prague for the period from July 23 to July 27\n",
    "2. Find four-star hotels in Paris\n",
    "3. book me the cheapest hotel in the center of Moscow for the period from August 12th to 19th, I'm just one guest\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "llm = ChatFireworks(model=\"accounts/fireworks/models/qwen2-72b-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = llm | NumberedListOutputParser()\n",
    "chain.invoke(system_generate_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "request_dataset = []\n",
    "\n",
    "# add samples from 72b model\n",
    "for i in range(20):\n",
    "    request_dataset.extend(\n",
    "        chain.invoke(system_generate_prompt)\n",
    "    )\n",
    "    time.sleep(.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and from 7b model\n",
    "llm = ChatFireworks(model=\"accounts/fireworks/models/qwen2-7b-instruct\")\n",
    "chain = llm | NumberedListOutputParser()\n",
    "\n",
    "for i in range(20):\n",
    "    request_dataset.extend(\n",
    "        chain.invoke(system_generate_prompt)\n",
    "    )\n",
    "    time.sleep(.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "shuffled_request_dataset = random.sample(request_dataset, len(request_dataset))\n",
    "\n",
    "with open(\"shuffled_request_dataset.json\", \"w\") as f:\n",
    "    json.dump(shuffled_request_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate gpt answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "search_hotels_system_prompt = \"\"\"\\\n",
    "Current time: 2024-06-18 17:45:12\n",
    "You are an assistant who can search and book a hotel for a user.\n",
    "Hotel search and reservations are made through the use of the following tools:\n",
    "\n",
    "{rendered_tools}\n",
    "\n",
    " Here is a list of possible parameters and their values:\n",
    "-location: str -> often just the name of the city\n",
    "-checkin_date: str -> format: YYYY-MM-DD\n",
    "-checkout_date: str -> format: YYYY-MM-DD\n",
    "-adults_number: int\n",
    "-children_number: int\n",
    "-min_rating: int -> must be in the range from 0 to 10\n",
    "-min_price: int\n",
    "-max_price: int\n",
    "-order_by: str -> possible values: popularity, price, rating\n",
    "-id: int -> it is used exclusively when booking, when the search has already been called\n",
    "\n",
    "Given the user input, return the name and input of the tool to use. Return your response as a JSON blob with 'name' and 'arguments' keys.\n",
    "\n",
    "The `arguments` should be a dictionary, with keys corresponding to the argument names and the values corresponding to the requested values.\n",
    "If the user has not provided some important information, you still need to send a json blob.\n",
    "Don't make up the argument values yourself! Take only what the user specified!\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", search_hotels_system_prompt),\n",
    "                (\"user\", \"{input}\"),\n",
    "            ]\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import render_text_description, book_hotel_tool, search_hotels_tool\n",
    "\n",
    "tools = [book_hotel_tool, search_hotels_tool]\n",
    "\n",
    "# generate answers from the 72b model\n",
    "llm = ChatFireworks(model=\"accounts/fireworks/models/qwen2-72b-instruct\")\n",
    "chain = prompt | llm\n",
    "rendered_tools = render_text_description(\n",
    "    tools, ignored_parameters=[\"data_loader\"]\n",
    ")\n",
    "\n",
    "gpt_outputs_72b = []\n",
    "\n",
    "for i, item in enumerate(shuffled_request_dataset):\n",
    "    llm_output = chain.invoke(\n",
    "                {\n",
    "                    \"input\": item,\n",
    "                    \"rendered_tools\": rendered_tools,\n",
    "                }\n",
    "            ).content\n",
    "\n",
    "    gpt_outputs_72b.append(llm_output)\n",
    "\n",
    "\n",
    "# and from 7b model\n",
    "llm = ChatFireworks(model=\"accounts/fireworks/models/qwen2-7b-instruct\")\n",
    "chain = prompt | llm\n",
    "\n",
    "gpt_outputs_7b = []\n",
    "\n",
    "for i, item in enumerate(shuffled_request_dataset):\n",
    "    llm_output = chain.invoke(\n",
    "                {\n",
    "                    \"input\": item,\n",
    "                    \"rendered_tools\": rendered_tools,\n",
    "                }\n",
    "            ).content\n",
    "\n",
    "    gpt_outputs_7b.append(llm_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import str_to_json\n",
    "\n",
    "gpt_outputs_7b_json = [str_to_json(x) for x in gpt_outputs_7b]\n",
    "gpt_outputs_72b_json = [str_to_json(x) for x in gpt_outputs_72b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_judgement_prompt = \"\"\"\n",
    "Current time: 2024-06-18 17:45:12\n",
    "You will be presented with a user request and two responses from different gpt.\n",
    "\n",
    "You need to write a new one based on these answers, either taking the best one or refining the best one.\n",
    "\n",
    "The responses will be in the form of a json blob.\n",
    "\n",
    "A common problem with current gpt responses is when different argument values are thought out.\n",
    "Examples:\n",
    "-The user did not specify the exact date, but gpt prescribed it.\n",
    "-The user did not specify the exact number of guests, but gpt thought of it and prescribed this value.\n",
    ", and so on..\n",
    "\n",
    "You must correct these errors, if they exist, and write a new, perfect answer.\n",
    "\n",
    "Here is a list of possible parameters and their values:\n",
    "-location: str -> often just the name of the city\n",
    "-checkin_date: str -> format: YYYY-MM-DD\n",
    "-checkout_date: str -> format: YYYY-MM-DD\n",
    "-adults_number: int\n",
    "-children_number: int\n",
    "-min_rating: int -> must be in the range from 0 to 10\n",
    "-min_price: int\n",
    "-max_price: int\n",
    "-order_by: str -> possible values: popularity, price, rating\n",
    "\n",
    "User request: {user_input}\n",
    "\n",
    "Answer 1: {answer_1}\n",
    "\n",
    "Answer 2: {answer_2}\n",
    "\n",
    "Your corrected version of the answer in json blob:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"user\", system_judgement_prompt),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "judge_answers = []\n",
    "\n",
    "\n",
    "for d1, d2, request in zip(gpt_outputs_7b_json, gpt_outputs_72b_json, shuffled_request_dataset):\n",
    "    try:\n",
    "        judge_answer = chain.invoke(\n",
    "            {\n",
    "                \"user_input\": request,\n",
    "                \"answer_1\": d1,\n",
    "                \"answer_2\": d2\n",
    "            }\n",
    "        ).content\n",
    "\n",
    "        judge_answers.append(str_to_json(judge_answer))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        judge_answer.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"judge_answers.json\", \"w\") as f:\n",
    "    json.dump(judge_answers, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "sig = inspect.signature(search_hotels_tool)\n",
    "search_parameters = sig.parameters\n",
    "\n",
    "search_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "corrected_judge_answers = []\n",
    "\n",
    "for judge_answer in judge_answers:\n",
    "    corrected_judge_answers.append(copy.deepcopy(judge_answer))\n",
    "    corrected_judge_answers[-1]['name'] = search_hotels_tool.__name__\n",
    "    for k, v in judge_answer['arguments'].items():\n",
    "        if k not in search_parameters:\n",
    "            corrected_judge_answers[-1]['arguments'].pop(k)\n",
    "    \n",
    "corrected_judge_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booking_dataset = []\n",
    "for request, judge_answer in zip(shuffled_request_dataset, corrected_judge_answers):\n",
    "    booking_dataset.append(\n",
    "        {\n",
    "            \"instruction\": request,\n",
    "            \"input\": \"\",\n",
    "            \"output\": str(judge_answer),\n",
    "        }\n",
    "    )\n",
    "\n",
    "with open(\"booking_dataset.json\", \"w\") as f:\n",
    "    json.dump(booking_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = booking_dataset[:int(len(booking_dataset)*.8)]\n",
    "val_data = booking_dataset[int(len(booking_dataset)*.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_data.json\", \"w\") as f:\n",
    "    json.dump(train_data, f, indent=4)\n",
    "\n",
    "with open(\"val_data.json\", \"w\") as f:\n",
    "    json.dump(val_data, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
